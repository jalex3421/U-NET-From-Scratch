{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **U-NET Implementation**\n",
        "\n",
        "AUTHOR: Alejandro Meza Tudela\n",
        "\n",
        "This notebook provides a complete, from-scratch implementation of U-Net, a foundational architecture for image segmentation."
      ],
      "metadata": {
        "id": "1ugoMxV7ZHxr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchsummary import summary"
      ],
      "metadata": {
        "id": "BaFl145HcLAj"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction"
      ],
      "metadata": {
        "id": "2SfiDXIWcRnz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "U-Net is a computer vision architecture that consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. At the time this architecture was introduced, it was praised for its huge performance, fast inference (often less than a second on a regular GPU), and its ability to perform well with a limited number of training samples.\n",
        "\n",
        "Note: To better understand this architecture, we recommend that you review the architecture diagram while looking at this code.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "8HSOaDVEcUiC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Code Implementation"
      ],
      "metadata": {
        "id": "N7K0doFed4Fk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The U-Net architecture will be built by creating several key building blocks, which will then be assembled to form the complete model."
      ],
      "metadata": {
        "id": "hwSfm5dTab1m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CONVOLUTION BLOCK\n",
        "The convolution block in the U-Net architecture consists of the repeated application of two 3x3 convolutions, each followed by a Rectified Linear Unit (ReLU) activation function."
      ],
      "metadata": {
        "id": "jwVGl87Iagx1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class convolution_block(nn.Module):\n",
        "    \"\"\"\n",
        "    A basic convolutional block used in the U-Net architecture.\n",
        "    Consists of two 3x3 convolutional layers, each followed by Batch Normalization.\n",
        "    A ReLU activation function is applied at the end.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_channels, output_channels):\n",
        "        super().__init__()\n",
        "        #define the operations inside the block\n",
        "        self.block = nn.Sequential(\n",
        "          nn.Conv2d(input_channels, output_channels, kernel_size=3, padding=1,bias=False),\n",
        "          nn.BatchNorm2d(output_channels),\n",
        "          nn.Conv2d(output_channels, output_channels, kernel_size=3, padding=1,bias=False),\n",
        "          nn.BatchNorm2d(output_channels),\n",
        "          nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "    def forward(self,x):\n",
        "      x = self.block(x)\n",
        "      return x"
      ],
      "metadata": {
        "id": "9sEbKkE4ZGc-"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CONTRACTING PATH DEFINITION\n",
        "\n",
        "The contracting path consists of a repetition of the convolution blocks and a 2x2 max pooling operation with stride 2 for downsampling. At each downsampling step, the number of feature channels is doubled."
      ],
      "metadata": {
        "id": "I0IsT2D1bcM3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class downsampling_block(nn.Module):\n",
        "  def __init__(self, input_channels, output_channels):\n",
        "    \"\"\"\n",
        "        Initializes the downsampling block.\n",
        "\n",
        "        Args:\n",
        "            input_channels (int): The number of channels in the input feature map.\n",
        "            output_channels (int): The number of channels after the convolution block.\n",
        "    \"\"\"\n",
        "    super().__init__()\n",
        "    self.conv_block = convolution_block(input_channels, output_channels)\n",
        "    self.pool = nn.MaxPool2d((2,2))\n",
        "\n",
        "  def forward(self,x):\n",
        "    x = self.conv_block(x)\n",
        "    p = self.pool(x)\n",
        "    # Return both the pre-pooled feature map for the skip connection\n",
        "    # and the pooled feature map for the next layer in the encoder.\n",
        "    return x,p"
      ],
      "metadata": {
        "id": "d_yZ-qWBbfy6"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### EXPANSIVE PATH DEFINITION\n",
        "\n",
        "Every step in the expansive path consists of an upsampling of the feature map, followed by a 2x2 \"up-convolution.\" This is followed by a concatenation with the correspondingly cropped feature map from the contracting path."
      ],
      "metadata": {
        "id": "p6JKbdh3dpuX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class upsampling_block(nn.Module):\n",
        "  \"\"\"\n",
        "    An upsampling block for the U-Net expansive path.\n",
        "    It combines upsampling with a skip connection from the encoder path.\n",
        "  \"\"\"\n",
        "  def __init__(self, input_channels, output_channels):\n",
        "    super().__init__()\n",
        "    #define the up-convolution\n",
        "    self.up_convolution = nn.ConvTranspose2d(input_channels, output_channels, kernel_size=2, stride=2, padding=0)\n",
        "    #define the convolution: current output + skip connection\n",
        "    self.convolution_block = convolution_block(\n",
        "                                               output_channels+output_channels, #current + skip connection\n",
        "                                               output_channels\n",
        "                                               )\n",
        "  def forward(self,x,skip_connection):\n",
        "    x = self.up_convolution(x)\n",
        "    #concatenate the previous result with the skipp connection before applying the convolution block\n",
        "    x = torch.cat([x,skip_connection],axis=1)\n",
        "    #apply the convolution as always\n",
        "    x = self.convolution_block(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "MN-dnf98drvD"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## UNET Architecture\n",
        "\n",
        "In summary, the U-Net architecture consists of two main components: a contracting path (encoder) and an expanding path (decoder), which together form a symmetric, U-shaped structure.\n",
        "\n",
        "- The contracting path is composed of 4 downsampling blocks, which progressively reduce the spatial dimensions while increasing the number of feature channels.\n",
        "\n",
        "- At the bottom of the U, there's a bottleneck layer that captures the most abstract representation of the input.\n",
        "\n",
        "- The expanding path consists of 4 upsampling blocks, which gradually recover the spatial dimensions and refine the segmentation output.\n",
        "\n",
        "![Alternative Text](https://idiotdeveloper.com/wp-content/uploads/2021/01/u-net-architecture.png)"
      ],
      "metadata": {
        "id": "k6n8HbxgfG36"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class UNET_Architecture(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    '''\n",
        "      CONTRACTING PATH\n",
        "    '''\n",
        "    self.encoder_1 = downsampling_block(3,64)\n",
        "    self.encoder_2 = downsampling_block(64,128)\n",
        "    self.encoder_3 = downsampling_block(128,256)\n",
        "    self.encoder_4 = downsampling_block(256,512)\n",
        "\n",
        "    '''\n",
        "      BOTTLENECK\n",
        "    '''\n",
        "    self.bottleneck = convolution_block(512,1024)\n",
        "\n",
        "    '''\n",
        "      EXPANSIVE PATH\n",
        "    '''\n",
        "    self.decoder_1 = upsampling_block(1024,512)\n",
        "    self.decoder_2 = upsampling_block(512,256)\n",
        "    self.decoder_3 = upsampling_block(256,128)\n",
        "    self.decoder_4 = upsampling_block(128,64)\n",
        "\n",
        "    '''\n",
        "      OUTPUT LAYER\n",
        "    '''\n",
        "    self.output_layer = nn.Conv2d(64,1,kernel_size=1)\n",
        "\n",
        "  def forward(self,x):\n",
        "    #contracting path\n",
        "    x1,p1 = self.encoder_1(x)\n",
        "    x2,p2 = self.encoder_2(p1)\n",
        "    x3,p3 = self.encoder_3(p2)\n",
        "    x4,p4 = self.encoder_4(p3)\n",
        "\n",
        "    #bottleneck\n",
        "    b = self.bottleneck(p4)\n",
        "\n",
        "    #expanding path\n",
        "    d1 = self.decoder_1(b,x4) #use of skip connection\n",
        "    d2 = self.decoder_2(d1,x3) #use of skip connection\n",
        "    d3 = self.decoder_3(d2,x2) #use of skip connection\n",
        "    d4 = self.decoder_4(d3,x1) #use of skip connection\n",
        "\n",
        "    #output layer\n",
        "    output = self.output_layer(d4)\n",
        "\n",
        "    return output"
      ],
      "metadata": {
        "id": "QGWB2cT8fMkG"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Try the implementation"
      ],
      "metadata": {
        "id": "gjrsO3vzoVVO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = UNET_Architecture() #create an instance of the model"
      ],
      "metadata": {
        "id": "wWUpgbsHo4c4"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#summary(model, input_size=(3, 256, 256)) #remove this comment to check some details of the model"
      ],
      "metadata": {
        "id": "psVtJteHo-pu"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After seeing some details of the model, let's create some random tensor to try it."
      ],
      "metadata": {
        "id": "CI0RW_7JpRoX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dummy_input = torch.randn(1, 3, 256, 256)\n",
        "print(f\"Input tensor shape: {dummy_input.shape}\")\n",
        "\n",
        "#Run the dummy input through the model\n",
        "try:\n",
        "  output = model(dummy_input)\n",
        "  # Print the output shape to verify everything worked\n",
        "  print(f\"Output tensor shape: {output.shape}\")\n",
        "\n",
        "except RuntimeError as e:\n",
        "  print(f\"An error occurred during the forward pass: {e}\")\n",
        "  print(\"This usually indicates a dimension mismatch somewhere in the model.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jx3V9xUdpbvG",
        "outputId": "327a299d-20cf-44d5-c87e-ef035bf07870"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input tensor shape: torch.Size([1, 3, 256, 256])\n",
            "Output tensor shape: torch.Size([1, 1, 256, 256])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The output shape confirms that:\n",
        "\n",
        "- The model is correctly assembled: The forward pass ran without any dimension mismatches or errors. This indicates that all the downsampling_blocks, bottleneck, and upsampling_blocks are correctly connected and that the skip connections are wired to the right places.\n",
        "\n",
        "- The output layer is working as intended: The final output tensor has a channel count of 1 and the same spatial dimensions (256x256) as the input. This is exactly what is needed for a binary segmentation task, where the single channel represents the logit for each pixel.\n",
        "\n",
        "So, all I can say, is that this U-NET model is ready to be trained! The implementation phase has been done succesfully."
      ],
      "metadata": {
        "id": "82rQJ7Z-ptBS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "References:\n",
        "- The U-Net architecture, proposed by Ronneberger et al. in 2015, is widely used for image segmentation ([paper link](https://arxiv.org/abs/1505.04597)).\n"
      ],
      "metadata": {
        "id": "SC8nubakq6LO"
      }
    }
  ]
}